{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcvNvlykvf1d//CihJOZlI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duruamobi/AAI2026/blob/main/Coding_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Data Source:\n",
        "# King County House Sales Dataset (Seattle area)\n",
        "# Kaggle: https://www.kaggle.com/datasets/harlfoxem/housesalesprediction\n",
        "# File used: kc_house_data.csv (downloaded from Kaggle)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Path to your uploaded zip - Corrected path\n",
        "zip_path = Path(\"/content/kc_house_data.csv.zip\")\n",
        "\n",
        "# Unzip into /mnt/data and find the CSV inside\n",
        "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "    z.extractall(\"/mnt/data\")\n",
        "    csv_names = [n for n in z.namelist() if n.lower().endswith(\".csv\")]\n",
        "    if not csv_names:\n",
        "        raise FileNotFoundError(\"No CSV found inside the ZIP file.\")\n",
        "    csv_path = Path(\"/mnt/data\") / csv_names[0]\n",
        "\n",
        "# Load real dataset\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Keep only required columns for Part 1\n",
        "df = df[['price', 'sqft_living', 'zipcode']]\n",
        "\n",
        "# Rename columns to match your original example\n",
        "df = df.rename(columns={\n",
        "    'sqft_living': 'square_footage',\n",
        "    'zipcode': 'location'\n",
        "})\n",
        "\n",
        "# Features and target\n",
        "X = df[['square_footage', 'location']]\n",
        "y = df['price']\n",
        "\n",
        "# Preprocessing: One-hot encode zipcode (location)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('location', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), ['location'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Create pipeline\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Split + train\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Example prediction: 2000 sqft in zipcode 98178\n",
        "new_house = pd.DataFrame({'square_footage': [2000], 'location': [98178]})\n",
        "predicted_price = model.predict(new_house)\n",
        "print(f\"Predicted price for a 2000 sq ft house in zipcode 98178: ${predicted_price[0]:,.2f}\")\n",
        "\n",
        "# Print $/sqft effect\n",
        "sqft_coef = model.named_steps['regressor'].coef_[-1]\n",
        "print(f\"\\nOn average, price increases by ${sqft_coef:.2f} per square foot (holding zipcode constant).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1NzZiRvVdWp",
        "outputId": "f70179f4-5309-49ae-9a67-2987bf5ba430"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted price for a 2000 sq ft house in zipcode 98178: $384,476.81\n",
            "\n",
            "On average, price increases by $250.93 per square foot (holding zipcode constant).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Data Source (Kaggle):\n",
        "# Downloaded from Kaggle (search on Kaggle: \"customer churn dataset\")\n",
        "# File in this project: customer_churn_dataset-testing-master.csv\n",
        "# (Replace the comment above with your exact Kaggle dataset link if required by your instructor.)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Load real dataset from your uploaded ZIP\n",
        "zip_path = Path(\"/content/customer_churn_dataset-testing-master.csv.zip\") # Corrected path\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "    z.extractall(\"/mnt/data\")\n",
        "    csv_names = [n for n in z.namelist() if n.lower().endswith(\".csv\")]\n",
        "    if not csv_names:\n",
        "        raise FileNotFoundError(\"No CSV found inside the ZIP file.\")\n",
        "    csv_path = Path(\"/mnt/data\") / csv_names[0]\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# ---- Select features that match Part 2 requirements ----\n",
        "# Demographics: Age, Gender\n",
        "# Usage patterns: Usage Frequency, Tenure\n",
        "# Purchase history: Total Spend\n",
        "# Customer service: Support Calls\n",
        "# Target: Churn (0/1)\n",
        "required_cols = [\n",
        "    \"Age\", \"Gender\", \"Tenure\", \"Usage Frequency\",\n",
        "    \"Support Calls\", \"Total Spend\", \"Churn\"\n",
        "]\n",
        "missing = [c for c in required_cols if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns in dataset: {missing}\")\n",
        "\n",
        "X = df[[\"Age\", \"Gender\", \"Tenure\", \"Usage Frequency\", \"Support Calls\", \"Total Spend\"]]\n",
        "y = df[\"Churn\"]\n",
        "\n",
        "# Preprocessing: scale numeric + one-hot encode categoricals\n",
        "num_features = [\"Age\", \"Tenure\", \"Usage Frequency\", \"Support Calls\", \"Total Spend\"]\n",
        "cat_features = [\"Gender\"]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_features),\n",
        "        (\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), cat_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", LogisticRegression(max_iter=2000, random_state=42))\n",
        "])\n",
        "\n",
        "# Split + train\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict churn probability for a new customer (example)\n",
        "new_customer = pd.DataFrame({\n",
        "    \"Age\": [35],\n",
        "    \"Gender\": [\"Male\"],          # must be a value used in your dataset (e.g., Male/Female)\n",
        "    \"Tenure\": [12],              # months (example)\n",
        "    \"Usage Frequency\": [20],     # frequency score (example)\n",
        "    \"Support Calls\": [5],\n",
        "    \"Total Spend\": [1500],       # total spend (example)\n",
        "})\n",
        "\n",
        "churn_probability = model.predict_proba(new_customer)[0][1]  # P(churn=1)\n",
        "\n",
        "threshold = 0.5\n",
        "churn_prediction = 1 if churn_probability > threshold else 0\n",
        "\n",
        "print(f\"Churn Probability for new customer: {churn_probability:.2f}\")\n",
        "print(f\"Churn Prediction (1 = churn, 0 = no churn): {churn_prediction}\")\n",
        "\n",
        "# Display model coefficients (log-odds)\n",
        "cat_feature_names = (\n",
        "    model.named_steps[\"preprocessor\"]\n",
        "         .named_transformers_[\"cat\"]\n",
        "         .get_feature_names_out(cat_features)\n",
        "         .tolist()\n",
        ")\n",
        "\n",
        "feature_names = num_features + cat_feature_names\n",
        "coefficients = model.named_steps[\"classifier\"].coef_[0]\n",
        "\n",
        "print(\"\\nModel Coefficients (log-odds):\")\n",
        "for feature, coef in zip(feature_names, coefficients):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4dj3_cSXTIo",
        "outputId": "42f88e00-f953-4730-f236-ac3a262ba935"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Churn Probability for new customer: 0.10\n",
            "Churn Prediction (1 = churn, 0 = no churn): 0\n",
            "\n",
            "Model Coefficients (log-odds):\n",
            "Age: 0.1511\n",
            "Tenure: 0.4499\n",
            "Usage Frequency: -0.2780\n",
            "Support Calls: 0.7140\n",
            "Total Spend: -0.1998\n",
            "Gender_Female: 0.3838\n",
            "Gender_Male: -0.4952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Data Source (Kaggle):\n",
        "# Downloaded from Kaggle (search: \"customer churn dataset\")\n",
        "# File used here: customer_churn_dataset-testing-master.csv\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Load real dataset from your uploaded ZIP\n",
        "zip_path = Path(\"/content/customer_churn_dataset-testing-master.csv.zip\")\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "    z.extractall(\"/mnt/data\")\n",
        "    csv_names = [n for n in z.namelist() if n.lower().endswith(\".csv\")]\n",
        "    if not csv_names:\n",
        "        raise FileNotFoundError(\"No CSV found inside the ZIP file.\")\n",
        "    csv_path = Path(\"/mnt/data\") / csv_names[0]\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# ---- Choose features for segmentation (NO churn label needed) ----\n",
        "# Demographics: Age\n",
        "# Buying/Spend: Total Spend\n",
        "# Behavior: Usage Frequency, Tenure\n",
        "# Customer service interactions: Support Calls\n",
        "features = [\"Age\", \"Total Spend\", \"Usage Frequency\", \"Tenure\", \"Support Calls\"]\n",
        "\n",
        "missing = [c for c in features if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required feature columns in dataset: {missing}\")\n",
        "\n",
        "# Drop missing values in selected features (simple cleanup)\n",
        "df_seg = df.dropna(subset=features).copy()\n",
        "\n",
        "X = df_seg[features]\n",
        "\n",
        "# Scale features (important for K-Means)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# ---- Determine optimal number of clusters using elbow method ----\n",
        "inertia = []\n",
        "K = range(1, 11)\n",
        "\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot elbow curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(list(K), inertia, 'bo-')\n",
        "plt.xlabel(\"Number of Clusters (K)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.title(\"Elbow Method for Optimal K\")\n",
        "plt.savefig(\"elbow_plot.png\")\n",
        "plt.close()\n",
        "\n",
        "# ---- Apply K-Means with chosen K ----\n",
        "# (You can change this after looking at elbow_plot.png)\n",
        "optimal_k = 3\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "df_seg[\"cluster\"] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# ---- Analyze clusters ----\n",
        "cluster_summary = df_seg.groupby(\"cluster\")[features].mean().round(2)\n",
        "print(\"Cluster Characteristics (Mean Values):\")\n",
        "print(cluster_summary)\n",
        "\n",
        "# Example targeted strategies (simple rule-based interpretation)\n",
        "for cluster in range(optimal_k):\n",
        "    print(f\"\\nCluster {cluster} Strategy:\")\n",
        "\n",
        "    avg_spend = cluster_summary.loc[cluster, \"Total Spend\"]\n",
        "    avg_usage = cluster_summary.loc[cluster, \"Usage Frequency\"]\n",
        "    avg_support = cluster_summary.loc[cluster, \"Support Calls\"]\n",
        "    avg_tenure = cluster_summary.loc[cluster, \"Tenure\"]\n",
        "\n",
        "    if avg_spend >= cluster_summary[\"Total Spend\"].quantile(0.66):\n",
        "        print(\"High-spending customers: Offer VIP perks, exclusive promotions, loyalty rewards.\")\n",
        "    elif avg_support >= cluster_summary[\"Support Calls\"].quantile(0.66):\n",
        "        print(\"High-support-need customers: Improve support experience, proactive check-ins, service credits.\")\n",
        "    elif avg_tenure <= cluster_summary[\"Tenure\"].quantile(0.33):\n",
        "        print(\"Newer customers: Focus on onboarding, education, early retention offers.\")\n",
        "    elif avg_usage >= cluster_summary[\"Usage Frequency\"].quantile(0.66):\n",
        "        print(\"Highly engaged customers: Upsell premium plans, bundles, referrals program.\")\n",
        "    else:\n",
        "        print(\"Moderate customers: Personalized offers based on interests and usage.\")\n",
        "\n",
        "# Save cluster assignments\n",
        "df_seg.to_csv(\"customer_segments.csv\", index=False)\n",
        "print(\"\\nSaved: elbow_plot.png and customer_segments.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OyRLmqvYmHc",
        "outputId": "c1c4bea6-23b2-41d1-cf88-d000bfb09ee7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Characteristics (Mean Values):\n",
            "           Age  Total Spend  Usage Frequency  Tenure  Support Calls\n",
            "cluster                                                            \n",
            "0        41.29       518.90            15.63   27.70           1.83\n",
            "1        44.13       553.20             6.94   32.85           7.09\n",
            "2        40.41       550.66            23.03   35.45           7.24\n",
            "\n",
            "Cluster 0 Strategy:\n",
            "Newer customers: Focus on onboarding, education, early retention offers.\n",
            "\n",
            "Cluster 1 Strategy:\n",
            "High-spending customers: Offer VIP perks, exclusive promotions, loyalty rewards.\n",
            "\n",
            "Cluster 2 Strategy:\n",
            "High-support-need customers: Improve support experience, proactive check-ins, service credits.\n",
            "\n",
            "Saved: elbow_plot.png and customer_segments.csv\n"
          ]
        }
      ]
    }
  ]
}